# headhunter-api-explorer

## Обзор проекта

End-to-end Data Engineering пайплайн для сбора и подготовки данных о рынке вакансий  
с акцентом на архитектуру, качество данных и compliance-ориентированные сценарии использования.

**Поток данных:**

HeadHunter API → Airflow → MinIO (Bronze) → Postgres → dbt (планируется) → BI (планируется)

Проект демонстрирует практический, приближённый к продакшену подход к построению data-платформ:
- слоистая архитектура  
- идемпотентные загрузки  
- наблюдаемость  
- обязательные проверки качества данных  

**Технологический стек:**
- Python  
- Apache Airflow  
- MinIO (S3-совместимое хранилище)  
- Postgres  
- Docker  
- dbt (планируется)

---

## Описание проекта

Проект реализует Data Engineering платформу для автоматизированного сбора, хранения  
и подготовки данных о вакансиях из HeadHunter API.

Архитектура построена по принципам, применяемым в реальных data-платформах:
- чёткое разделение ingestion и трансформаций  
- хранение raw-данных как источника истины  
- воспроизводимые и безопасные повторные запуски пайплайнов  

Помимо классической аналитики рынка труда, проект ориентирован на **Compliance и RegTech-кейсы**:
- анализ работодателей  
- анализ отраслей и географии  
- выявление потенциально подозрительных формулировок в описаниях вакансий  

---

## Архитектура пайплайна

```text
HeadHunter API
      ↓
Airflow (DAG-и ingestion)
      ↓
MinIO (Bronze слой, JSONL, партиционирование по дате)
      ↓
Airflow (загрузка и проверки)
      ↓
Postgres (raw / staging)
      ↓
dbt (Silver / Gold слои) — планируется
      ↓
BI / дашборды — планируется
```

---

## Архитектурные принципы

- Bronze слой — **source of truth**
- Raw-данные не модифицируются после загрузки
- Ingestion строго отделён от трансформаций
- Daily-загрузки отделены от init / backfill
- Все пайплайны идемпотентны
- Контроль покрытия и качества данных обязателен
- Обогащённые записи self-contained и трассируемы

---

## Загрузка данных из HeadHunter API
## Список вакансий
На первом этапе формируются списки вакансий на основе search-профилей.
Эти датасеты используются как **manifest** для последующего обогащения и задают
ожидаемый объём данных для каждой загрузки.

## Детали вакансий (реализовано)
Для каждого vacancy_id выполняется запрос к эндпоинту:

```text
/vacancies/{id}
```
После чего данные сохраняются отдельным Bronze-датасетом.

### Загружаемые атрибуты включают:

- полное описание вакансии
- ключевые навыки
- профессиональные роли и специализации
- адрес, метро и географические координаты (при наличии)
- расширенные данные работодателя
- поведенческие маркеры (premium, тесты, требования к отклику)
- временные метки жизненного цикла вакансии (created, published, archived)

Детали вакансий хранятся отдельно от списков, формируя обогащённый Bronze-слой,
пригодный для текстового анализа, compliance-проверок и downstream-трансформаций.

---

## Качество данных и наблюдаемость
### Реализовано
- Проверка **expected vs loaded** (покрытие данных)
- Логирование на уровне батчей
- Классификация HTTP-ошибок:
  - not_found (404)
  - rate_limited (429)
  - http_error
  - request_exception
  - json_decode_error
- Отдельный Bronze-датасет с ошибочными записями
- Определение severity каждого прогона: OK, WARNING, CRITICAL
- Идемпотентная логика загрузки с безопасным повторным запуском

### Пример логов:

```text
[batch 007] ok=199 failed=1 status_counts={404: 1}
[TOTAL] expected=2803 ok=2793 failed=10 severity=WARNING
```
##  Airflow DAG-и
| DAG | Тип запуска | Назначение |
|-----|------------|------------|
| `aida_hh_init_bronze_json` | manual | Первичная загрузка и backfill вакансий |
| `aida_hh_daily_json` | scheduled (daily) | Инкрементальная загрузка списков вакансий |
| `aida_hh_details_daily` | scheduled (daily) | Загрузка и обогащение деталей вакансий |
| `nataliia_hh_to_postgres` | scheduled (daily) | Перенос Bronze-данных в Postgres |
| `sanctions_load` | planned | Загрузка санкционных списков |
| `dbt_transform_run` | planned | dbt-модели Silver / Gold и тесты |

---

##  Структура репозитория
```text
.
├── dags/
│   ├── aida_hh_init_*.py
│   ├── aida_hh_daily_*.py
│   ├── aida_hh_details_daily.py
│   ├── utils/
│   └── legacy/
├── configs/
│   └── search_profiles.yaml
├── docker-compose.yml
├── Dockerfile
├── .env.example
├── logs/
└── minio_data/
```
##  Команда и зоны ответственности
### Aida Iskakova
- Архитектура пайплайна
- Ingestion: HH API → MinIO (Bronze)
- Обогащение данных деталей вакансий
- Контроль качества и покрытия данных
- Формирование compliance-требований к данным

### Natalia Tarasova
- Загрузка данных из Bronze в Postgres
- dbt-модели Silver / Gold
- Агрегации и аналитические датасеты
- Мониторинг и алерты

---

## Roadmap
- Загрузка санкционных списков (EU / UK)
- dbt-модели Silver и Gold
- Compliance-ориентированные витрины данных
- BI-дашборды (Metabase / Superset)
- Расширенные проверки качества данных через dbt

---

## Итог
Проект демонстрирует системный и приближённый к продакшену подход
к построению Data Engineering платформы с акцентом на архитектуру,
надёжность и качество данных.

Репозиторий отражает практики командной разработки, наблюдаемость пайплайнов
и готовность данных к аналитическим и compliance-сценариям использования.
