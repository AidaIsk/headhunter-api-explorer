headhunter-api-explorer
Обзор проекта

End-to-end Data Engineering пайплайн для сбора и подготовки данных о рынке вакансий с акцентом на архитектуру, качество данных и compliance-ориентированные сценарии использования.

Поток данных:
HeadHunter API → Airflow → MinIO (Bronze) → Postgres → dbt (планируется) → BI (планируется)

Проект демонстрирует практический, приближенный к продакшену подход к построению data-платформ: слоистая архитектура, идемпотентные загрузки, наблюдаемость и обязательные проверки качества данных.

Технологический стек:
Python, Apache Airflow, MinIO (S3-совместимое хранилище), Postgres, Docker, dbt (планируется)

Описание проекта

Проект реализует Data Engineering платформу для автоматизированного сбора, хранения и подготовки данных о вакансиях из HeadHunter API.

Архитектура построена по принципам, применяемым в реальных data-платформах:

чёткое разделение ingestion и трансформаций,

сохранение raw-данных как источника истины,

воспроизводимые и безопасные повторные запуски пайплайнов.

Помимо классической аналитики рынка труда, проект ориентирован на compliance и RegTech-кейсы: анализ работодателей, отраслей, географии и потенциально подозрительных формулировок в описаниях вакансий.

Архитектура пайплайна
HeadHunter API
      ↓
Airflow (DAG-и ingestion)
      ↓
MinIO (Bronze слой, JSONL, партиционирование по дате)
      ↓
Airflow (загрузка и проверки)
      ↓
Postgres (raw / staging)
      ↓
dbt (Silver / Gold слои) — планируется
      ↓
BI / дашборды — планируется

Архитектурные принципы

Bronze слой — source of truth

Raw-данные не модифицируются после загрузки

Ingestion строго отделён от трансформаций

Daily-загрузки отделены от init / backfill

Все пайплайны идемпотентны

Контроль покрытия и качества данных обязателен

Обогащённые записи self-contained и трассируемы

Загрузка данных из HeadHunter API
Список вакансий

На первом этапе формируются списки вакансий на основе search-профилей. Эти датасеты используются как manifest для последующего обогащения и задают ожидаемый объём данных для каждой загрузки.

Детали вакансий (реализовано)

Для каждого vacancy_id выполняется запрос к эндпоинту /vacancies/{id}, после чего данные сохраняются отдельным Bronze-датасетом.

Загружаемые атрибуты включают:

Полное описание вакансии

Ключевые навыки

Профессиональные роли и специализации

Адрес, метро и географические координаты (при наличии)

Расширенные данные работодателя

Поведенческие маркеры (premium, тесты, требования к отклику)

Временные метки жизненного цикла вакансии (created, published, archived)

Детали вакансий хранятся отдельно от списков, формируя обогащённый Bronze-слой, пригодный для текстового анализа, compliance-проверок и downstream-трансформаций.

Качество данных и наблюдаемость
Реализовано

Проверка expected vs loaded (покрытие данных)

Логирование на уровне батчей

Классификация ошибок HTTP:

not_found (404)

rate_limited (429)

http_error

request_exception

json_decode_error

Отдельный Bronze-датасет с ошибочными записями

Определение severity каждого прогона: OK, WARNING, CRITICAL

Идемпотентная логика загрузки с безопасным повторным запуском

Пример логов
[batch 007] ok=199 failed=1 status_counts={404: 1}
[TOTAL] expected=2803 ok=2793 failed=10 severity=WARNING

Airflow DAG-и
DAG	Расписание	Назначение
aida_hh_init_bronze_json	manual	Первичная / backfill загрузка вакансий
aida_hh_daily_json	daily	Ежедневная инкрементальная загрузка вакансий
aida_hh_details_daily	daily	Загрузка и обогащение деталей вакансий
nataliia_hh_to_postgres	daily	Загрузка Bronze-данных в Postgres
sanctions_load	планируется	Загрузка санкционных списков (EU / UK)
dbt_transform_run	планируется	Silver / Gold трансформации и тесты
Структура репозитория
.
├── dags/
│   ├── aida_hh_init_*.py
│   ├── aida_hh_daily_*.py
│   ├── aida_hh_details_daily.py
│   ├── utils/
│   └── legacy/
├── configs/
│   └── search_profiles.yaml
├── docker-compose.yml
├── Dockerfile
├── .env.example
├── logs/
└── minio_data/

Команда и зоны ответственности

Aida Iskakova

Архитектура пайплайна

Ingestion: HH API → MinIO (Bronze)

Обогащение данных деталей вакансий

Контроль качества и покрытия данных

Формирование compliance-требований к данным

Natalia Tarasova

Загрузка данных из Bronze в Postgres

dbt-модели Silver / Gold

Агрегации и аналитические датасеты

Мониторинг и алерты

Roadmap

Загрузка санкционных списков (EU / UK)

dbt-модели Silver и Gold

Compliance-ориентированные витрины данных

BI-дашборды (Metabase / Superset)

Расширенные проверки качества данных через dbt

Итог

Проект демонстрирует системный и приближенный к продакшену подход к построению Data Engineering платформы с акцентом на архитектуру, надёжность и качество данных.

Репозиторий отражает практики командной разработки, наблюдаемость пайплайнов и готовность данных к аналитическим и compliance-сценариям использования.
