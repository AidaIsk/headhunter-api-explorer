# headhunter-api-explorer

## Обзор проекта

End-to-end Data Engineering пайплайн для сбора и подготовки данных о рынке вакансий  
с акцентом на архитектуру, качество данных и compliance-ориентированные сценарии использования.

### Поток данных

**HeadHunter API → Airflow → MinIO (Bronze) → Postgres → dbt (планируется) → BI (планируется)**

Проект демонстрирует практический, приближенный к продакшену подход  
к построению data-платформ:

- слоистая архитектура  
- идемпотентные загрузки  
- наблюдаемость  
- обязательные проверки качества данных  

### Технологический стек

- Python  
- Apache Airflow  
- MinIO (S3-совместимое хранилище)  
- Postgres  
- Docker  
- dbt (планируется)  

---

## Описание проекта

Проект реализует Data Engineering платформу для автоматизированного  
сбора, хранения и подготовки данных о вакансиях из HeadHunter API.

Архитектура построена по принципам, применяемым в реальных data-платформах:

- чёткое разделение ingestion и трансформаций  
- сохранение raw-данных как источника истины  
- воспроизводимые и безопасные повторные запуски пайплайнов  

Помимо классической аналитики рынка труда, проект ориентирован  
на compliance и RegTech-кейсы:

- анализ работодателей  
- анализ отраслей  
- анализ географии  
- выявление потенциально подозрительных формулировок в описаниях вакансий  

---

## Архитектура пайплайна

```text
HeadHunter API
      ↓
Airflow (DAG-и ingestion)
      ↓
MinIO (Bronze слой, JSONL, партиционирование по дате)
      ↓
Airflow (загрузка и проверки)
      ↓
Postgres (raw / staging)
      ↓
dbt (Silver / Gold слои) — планируется
      ↓
BI / дашборды — планируется
Архитектурные принципы
Bronze слой — source of truth

Raw-данные не модифицируются после загрузки

Ingestion строго отделён от трансформаций

Daily-загрузки отделены от init / backfill

Все пайплайны идемпотентны

Контроль покрытия и качества данных обязателен

Обогащённые записи self-contained и трассируемы

Загрузка данных из HeadHunter API
Список вакансий
На первом этапе формируются списки вакансий на основе search-профилей.
Эти датасеты используются как manifest для последующего обогащения
и задают ожидаемый объём данных для каждой загрузки.

Детали вакансий (реализовано)
Для каждого vacancy_id выполняется запрос
к эндпоинту /vacancies/{id}, после чего данные сохраняются
отдельным Bronze-датасетом.

Загружаемые атрибуты включают:

полное описание вакансии

ключевые навыки

профессиональные роли и специализации

адрес, метро и географические координаты (при наличии)

расширенные данные работодателя

поведенческие маркеры (premium, тесты, требования к отклику)

временные метки жизненного цикла вакансии
(created, published, archived)

Детали вакансий хранятся отдельно от списков,
формируя обогащённый Bronze-слой, пригодный для:

текстового анализа

compliance-проверок

downstream-трансформаций

Качество данных и наблюдаемость
Реализовано
проверка expected vs loaded (покрытие данных)

логирование на уровне батчей

классификация ошибок HTTP:

not_found (404)

rate_limited (429)

http_error

request_exception

json_decode_error

отдельный Bronze-датасет с ошибочными записями

определение severity каждого прогона: OK, WARNING, CRITICAL

идемпотентная логика загрузки с безопасным повторным запуском

Пример логов
text
Копировать код
[batch 007] ok=199 failed=1 status_counts={404: 1}
[TOTAL] expected=2803 ok=2793 failed=10 severity=WARNING
Airflow DAG-и
DAG	Расписание	Назначение
aida_hh_init_bronze_json	manual	Первичная / backfill загрузка вакансий
aida_hh_daily_json	daily	Ежедневная инкрементальная загрузка вакансий
aida_hh_details_daily	daily	Загрузка и обогащение деталей вакансий
nataliia_hh_to_postgres	daily	Загрузка Bronze-данных в Postgres
sanctions_load	планируется	Загрузка санкционных списков (EU / UK)
dbt_transform_run	планируется	Silver / Gold трансформации и тесты

Структура репозитория
text
Копировать код
.
├── dags/
│   ├── aida_hh_init_*.py
│   ├── aida_hh_daily_*.py
│   ├── aida_hh_details_daily.py
│   ├── utils/
│   └── legacy/
├── configs/
│   └── search_profiles.yaml
├── docker-compose.yml
├── Dockerfile
├── .env.example
├── logs/
└── minio_data/
Команда и зоны ответственности
Aida Iskakova
архитектура пайплайна

ingestion: HH API → MinIO (Bronze)

обогащение данных деталей вакансий

контроль качества и покрытия данных

формирование compliance-требований к данным

Natalia Tarasova
загрузка данных из Bronze в Postgres

dbt-модели Silver / Gold

агрегации и аналитические датасеты

мониторинг и алерты

Roadmap
загрузка санкционных списков (EU / UK)

dbt-модели Silver и Gold

compliance-ориентированные витрины данных

BI-дашборды (Metabase / Superset)

расширенные проверки качества данных через dbt

Итог
Проект демонстрирует системный и приближенный к продакшену подход
к построению Data Engineering платформы с акцентом
на архитектуру, надёжность и качество данных.

Репозиторий отражает практики командной разработки,
наблюдаемость пайплайнов и готовность данных
к аналитическим и compliance-сценариям использования.
