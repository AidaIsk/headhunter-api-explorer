import requests
import pandas as pd
import os
from datetime import datetime, timedelta, timezone

# –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∑–∫–∏—Ö–∫ –∏–Ω–∂–µ–Ω–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–π
KEYWORDS = [
    "data engineer",
    "–∏–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö",
    "sql —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
    "dba",
    "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö",
    "etl",
    "bi analyst",
]

AREAS_URL = "https://api.hh.ru/areas"
VACANCIES_URL = "https://api.hh.ru/vacancies"
# –≤ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–µ–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞—Å –∫–∞–∫ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
HEADERS = {"User-Agent": "Mozilla/5.0"}

# –ø–æ —ç—Ç–æ–º—É –ø—É—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ airflow webserver –±—É–¥–µ—Ç –ø–æ–º–µ—â–µ–Ω –¥–∞–≥
CSV_PATH = "/opt/airflow/dags/data/hh_vacancies.csv"


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def get_regions():
    """–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –†–æ—Å—Å–∏–∏."""
    resp = requests.get(AREAS_URL, headers=HEADERS)
    resp.raise_for_status()
    # –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–≥–æ–Ω–æ–≤ –†–æ—Å—Å–∏–∏; next –±–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞.
    russia = next(a for a in resp.json() if a["name"] == "–†–æ—Å—Å–∏—è")
    # –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –±–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π 
    return [region for region in russia["areas"] if not region["areas"]]


def fetch_vacancies(region_id, keyword, start_date, end_date):
    """–í—ã–≥—Ä—É–∑–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ —Ä–µ–≥–∏–æ–Ω—É –∏ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É –∑–∞ –ø–µ—Ä–∏–æ–¥."""
    # —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π
    vacancies = []
    # —Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞ HH
    page = 0

    # —Å–æ–∑–¥–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–∫–∞ –Ω–µ –ø–µ—Ä–µ–±–µ—Ä–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –í—ã–π–¥–µ–º –∏–∑ —Ü–∏–∫–ª–∞ —á–µ—Ä–µ–∑ break
    while True:
        params = {
            "text": keyword,
            "area": region_id,
            "per_page": 100,
            "page": page,
            "date_from": start_date.isoformat(),
            "date_to": end_date.isoformat(),
        }
        resp = requests.get(VACANCIES_URL, headers=HEADERS, params=params)
        # –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞, –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –æ—Ç–∫–∞–∑–∞–ª –Ω–∞–º –ø–æ –ø—Ä–∏—á–∏–Ω–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω—É–∂–Ω—ã—Ö –ø—Ä–∞–≤
        if resp.status_code == 403:
            break
        resp.raise_for_status()
        data = resp.json()

        for v in data.get("items", []):
            vacancies.append({
                "id": v["id"],
                "name": v["name"],
                "region": v["area"]["name"],
                "keyword": keyword,
                "employer": v["employer"]["name"] if v.get("employer") else None,
                "published_at": v["published_at"],
                "url": v["alternate_url"],
                "salary_from": v["salary"]["from"] if v.get("salary") else None,
                "salary_to": v["salary"]["to"] if v.get("salary") else None,
                "currency": v["salary"]["currency"] if v.get("salary") else None
            })

        # –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞, –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –±–æ–ª—å—à–µ –Ω–µ—Ç
        if page >= data.get("pages", 1) - 1:
            break
        page += 1

    return vacancies


'''def save_to_csv(vacancies, mode="w"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ CSV (append –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞)."""
    df = pd.DataFrame(vacancies)
    if not df.empty:
        # –ø–∏—à–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, –µ—Å–ª–∏ —Ä–µ–∂–∏–º –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–∞–π–ª–∞ - –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å
        df.to_csv(CSV_PATH, mode=mode, header=(mode == "w"), index=False)'''

def save_to_csv(vacancies, mode="w"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ CSV, –ø—Ä–∏ append –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ (–ø–æ id+name)."""
    df = pd.DataFrame(vacancies)

    if df.empty:
        return

    # –¥–∞—Ç–∞ –≤—ã–≥—Ä—É–∑–∫–∏
    df["update_at"] = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º id –∫ —Å—Ç—Ä–æ–∫–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ merge
    df["id"] = df["id"].astype(str)

    if mode == "a" and os.path.exists(CSV_PATH):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        old_df = pd.read_csv(CSV_PATH, usecols=["id", "name"])
        old_df["id"] = old_df["id"].astype(str)

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
        df_new = df.merge(old_df, on=["id", "name"], how="left", indicator=True)
        df_new = df_new[df_new["_merge"] == "left_only"].drop(columns=["_merge"])

        if df_new.empty:
            print("‚úÖ –ù–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω—ë–Ω")
            return

        # –î–æ–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        df_new.to_csv(CSV_PATH, mode="a", header=False, index=False)
        print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ {len(df_new)} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫")

    else:
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ mode="w"
        df.to_csv(CSV_PATH, mode="w", header=True, index=False)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ (–Ω–æ–≤—ã–π —Ñ–∞–π–ª)")




# --- –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è DAG ---
def initial_load():
    """–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π."""
    regions = get_regions()
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=30)
    all_vacancies = []

# –∏—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –∫–∞–∂–¥–æ–º —Ä–µ–≥–∏–æ–Ω–µ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for region in regions:
        for kw in KEYWORDS:
            all_vacancies.extend(fetch_vacancies(region["id"], kw, start_date, end_date))

    save_to_csv(all_vacancies, mode="w")


def incremental_load():
    """–î–æ–≥—Ä—É–∑–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏."""
    regions = get_regions()
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=1)
    all_vacancies = []

# –∏—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –∫–∞–∂–¥–æ–º —Ä–µ–≥–∏–æ–Ω–µ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for region in regions:
        for kw in KEYWORDS:
            all_vacancies.extend(fetch_vacancies(region["id"], kw, start_date, end_date))

    save_to_csv(all_vacancies, mode="a")